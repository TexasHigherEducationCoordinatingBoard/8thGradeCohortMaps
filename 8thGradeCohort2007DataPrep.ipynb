{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This program prepares the selected data from 2007 8th Grade Cohort Longitudinal Study for mapping\n",
    "\n",
    "### Begin by downloading the Cohort Workbook from [the THECB website](http://www.txhighereddata.org/index.cfm?objectId=F2CBE4A0-C90B-11E5-8D610050560100A9). \n",
    "\n",
    "The selected data focuses on target populations from the Texas Higher Education Strategic Plan. The target populations examined here are African American students, particularly African american male students, and Hispanic students.\n",
    "\n",
    "### Target populations are examined by TEA Region which is how the data is presented by the THECB cohort workbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import arcpy\n",
    "import io\n",
    "import os\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by downloading the 2007 cohort workbook from the THECB website\n",
    "\n",
    "The cohort workbooks are available at: http://www.txhighereddata.org/index.cfm?objectId=F2CBE4A0-C90B-11E5-8D610050560100A9\n",
    "\n",
    "Save the workbook in 'Data\\8th Grade FY2007 Cohort Workbook.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First get the Gender by Ethnicity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TEAReg         RegName  AAmCoho  AAmnEnr    AAmpEnr  AAmnComp   AAmpComp\n",
      "4        1        Edinburg     27.0     16.0  59.259259       4.0  14.814815\n",
      "12       2  Corpus Christi    160.0     70.0  43.750000      16.0  10.000000\n",
      "20       3        Victoria    237.0     98.0  41.350211      31.0  13.080169\n",
      "28       4         Houston   8646.0   4358.0  50.404811     922.0  10.663891\n",
      "36       5        Beaumont    949.0    429.0  45.205479      73.0   7.692308\n",
      "..     ...             ...      ...      ...        ...       ...        ...\n",
      "124     16        Amarillo    178.0     80.0  44.943820      14.0   7.865169\n",
      "132     17         Lubbock    252.0     95.0  37.698413      16.0   6.349206\n",
      "140     18         Midland    171.0     58.0  33.918129      14.0   8.187135\n",
      "148     19         El Paso    204.0     93.0  45.588235      21.0  10.294118\n",
      "156     20     San Antonio   1070.0    513.0  47.943925     118.0  11.028037\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\8th Grade FY2007 Cohort Workbook.xlsx', sheet_name='TEA by Gender by Ethnicity', header=None, index_col=None, skiprows=6)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2,3,4,17,18,21,22]]\n",
    "\n",
    "#Drop the rows I don't need\n",
    "GenEth=xl2[:160]\n",
    "GenEth.columns=['TEAReg','RegName','Gender','Eth', 'CohoN', 'nEnr', 'pEnr', 'nComp', 'pComp']\n",
    "\n",
    "#Make Dataset just of African American Males (60x30TX target popultation - this wasn't used for the maps)\n",
    "AAmales=GenEth.loc[(GenEth['Eth']=='African American') & (GenEth['Gender']=='Male')].copy() #copy to avoid chained indexing\n",
    "AAmales=AAmales.drop(['Gender','Eth'], axis=1) #Keep the columns I need\n",
    "AAmales.columns=['TEAReg','RegName','AAmCoho', 'AAmnEnr', 'AAmpEnr', 'AAmnComp', 'AAmpComp']\n",
    "AAmales['AAmpEnr']=100*AAmales['AAmpEnr']\n",
    "AAmales['AAmpComp']=100*AAmales['AAmpComp']\n",
    "\n",
    "print(AAmales)\n",
    "#AAmales.to_csv('AAmales.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### African American and Hispanic totals by region. \n",
    "For this, we'll collapse on ethicity to remove gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TEAReg         RegName   AACoho  AAnEnr  AAnComp     AApEnr    AApComp\n",
      "0        1        Edinburg     60.0    35.0     12.0  58.333333  20.000000\n",
      "1        2  Corpus Christi    306.0   151.0     40.0  49.346405  13.071895\n",
      "2        3        Victoria    447.0   222.0     71.0  49.664430  15.883669\n",
      "3        4         Houston  17218.0  9734.0   2546.0  56.533860  14.786851\n",
      "4        5        Beaumont   1813.0   957.0    204.0  52.785438  11.252068\n",
      "..     ...             ...      ...     ...      ...        ...        ...\n",
      "15      16        Amarillo    364.0   184.0     38.0  50.549451  10.439560\n",
      "16      17         Lubbock    480.0   202.0     31.0  42.083333   6.458333\n",
      "17      18         Midland    343.0   143.0     39.0  41.690962  11.370262\n",
      "18      19         El Paso    380.0   170.0     42.0  44.736842  11.052632\n",
      "19      20     San Antonio   2074.0  1095.0    342.0  52.796528  16.489875\n",
      "\n",
      "[20 rows x 7 columns]\n",
      "    TEAReg         RegName  HisCoho  HisnEnr  HisnComp    HispEnr   HispComp\n",
      "0        1        Edinburg  25584.0  14376.0    5405.0  56.191370  21.126485\n",
      "1        2  Corpus Christi   5423.0   2557.0     792.0  47.151023  14.604462\n",
      "2        3        Victoria   1821.0    736.0     229.0  40.417353  12.575508\n",
      "3        4         Houston  30026.0  13183.0    4378.0  43.905282  14.580697\n",
      "4        5        Beaumont    652.0    270.0     105.0  41.411043  16.104294\n",
      "..     ...             ...      ...      ...       ...        ...        ...\n",
      "15      16        Amarillo   2213.0    987.0     350.0  44.600090  15.815635\n",
      "16      17         Lubbock   2819.0   1145.0     351.0  40.617240  12.451224\n",
      "17      18         Midland   3393.0   1429.0     424.0  42.116121  12.496316\n",
      "18      19         El Paso  11690.0   6927.0    2290.0  59.255774  19.589393\n",
      "19      20     San Antonio  18037.0   8618.0    2896.0  47.779564  16.055885\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Keep Hispanic and African American counts, collapse to remove gender, and then recalculate percents \n",
    "EthCounts=GenEth.drop(GenEth.columns[[2,6,8]], axis=1) #axis=0 for rows, axis=1 for columns\n",
    "#AA_Hisp=EthCounts.loc[EthCounts['Eth'].isin(['African American', 'Hispanic'])]\n",
    "#AA_Hisp_collapsed=AA_Hisp.groupby([\"TEAReg\", \"RegName\",\"Eth\"]).sum()\n",
    "\n",
    "\n",
    "#Make African American Group\n",
    "AAtemp=EthCounts.loc[EthCounts['Eth']=='African American'].copy() #copy to avoid chained indexing\n",
    "AA=AAtemp.groupby([\"TEAReg\", \"RegName\",\"Eth\"], as_index=False).sum()\n",
    "AA['AApEnr']=100*AA['nEnr']/AA['CohoN']\n",
    "AA['AApComp']=100*AA['nComp']/AA['CohoN']\n",
    "AA=AA.drop(['Eth'], axis=1) #Keep the columns I need\n",
    "AA.columns=['TEAReg','RegName','AACoho', 'AAnEnr','AAnComp','AApEnr','AApComp']\n",
    "\n",
    "#Make Hispanic Group\n",
    "Hisptemp=EthCounts.loc[EthCounts['Eth']=='Hispanic'].copy() #copy to avoid chained indexing\n",
    "Hisp=Hisptemp.groupby([\"TEAReg\", \"RegName\",\"Eth\"], as_index=False).sum()\n",
    "Hisp['HispEnr']=100*Hisp['nEnr']/Hisp['CohoN']\n",
    "Hisp['HispComp']=100*Hisp['nComp']/Hisp['CohoN']\n",
    "Hisp=Hisp.drop(['Eth'], axis=1) #Keep the columns I need\n",
    "Hisp.columns=['TEAReg','RegName','HisCoho', 'HisnEnr','HisnComp','HispEnr','HispComp']\n",
    "\n",
    "\n",
    "#print(AA_Hisp_collapsed)\n",
    "print(AA)\n",
    "print(Hisp)\n",
    "#AA.to_csv('AA.csv')\n",
    "#Hisp.to_csv('Hisp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Male enrollment and completion by region. \n",
    "For this, we'll collapse on gender and remove ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TEAReg         RegName  TotmCoho  TotmnEnr  TotmnComp   TotmpEnr  \\\n",
      "0        1        Edinburg   13488.0    7135.0     2411.0  52.898873   \n",
      "1        2  Corpus Christi    4113.0    1857.0      601.0  45.149526   \n",
      "2        3        Victoria    2144.0    1008.0      435.0  47.014925   \n",
      "3        4         Houston   37900.0   19325.0     6852.0  50.989446   \n",
      "4        5        Beaumont    3199.0    1503.0      525.0  46.983432   \n",
      "..     ...             ...       ...       ...        ...        ...   \n",
      "15      16        Amarillo    2987.0    1443.0      515.0  48.309340   \n",
      "16      17         Lubbock    2899.0    1361.0      498.0  46.947223   \n",
      "17      18         Midland    3005.0    1311.0      433.0  43.627288   \n",
      "18      19         El Paso    6775.0    3673.0     1041.0  54.214022   \n",
      "19      20     San Antonio   14188.0    6711.0     2407.0  47.300536   \n",
      "\n",
      "    TotmpComp  \n",
      "0   17.875148  \n",
      "1   14.612205  \n",
      "2   20.289179  \n",
      "3   18.079156  \n",
      "4   16.411379  \n",
      "..        ...  \n",
      "15  17.241379  \n",
      "16  17.178337  \n",
      "17  14.409318  \n",
      "18  15.365314  \n",
      "19  16.965041  \n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Get total male counts by region, collape on gender, counts only.\n",
    "GenCounts=GenEth.drop(GenEth.columns[[3,6,8]], axis=1) #axis=0 for rows, axis=1 for columns\n",
    "Allmalestemp=GenCounts.loc[GenCounts['Gender']=='Male'].copy() #copy to avoid chained indexing\n",
    "Allmales=Allmalestemp.groupby([\"TEAReg\", \"RegName\"], as_index=False).sum().copy()\n",
    "Allmales['AllmpEnr']=100*Allmales['nEnr']/Allmales['CohoN']\n",
    "Allmales['AllmpComp']=100*Allmales['nComp']/Allmales['CohoN']\n",
    "Allmales.columns=['TEAReg', 'RegName','TotmCoho', 'TotmnEnr','TotmnComp','TotmpEnr','TotmpComp']\n",
    "\n",
    "\n",
    "print(Allmales)\n",
    "#Allmales.to_csv('Allmales.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Economic Disadvantaged student data by region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEAReg         RegName  EcoCoho  EconEnr    EcopEnr  EconComp   EcopComp\n",
      "1       1        Edinburg  22752.0  12222.0  53.718354    4377.0  19.237869\n",
      "3       2  Corpus Christi   4635.0   1816.0  39.180151     460.0   9.924488\n",
      "5       3        Victoria   2153.0    833.0  38.690200     250.0  11.611705\n",
      "7       4         Houston  37985.0  16583.0  43.656707    4718.0  12.420692\n",
      "9       5        Beaumont   3056.0   1260.0  41.230366     304.0   9.947644\n",
      "..    ...             ...      ...      ...        ...       ...        ...\n",
      "31     16        Amarillo   3005.0   1280.0  42.595674     393.0  13.078203\n",
      "33     17         Lubbock   3198.0   1211.0  37.867417     329.0  10.287680\n",
      "35     18         Midland   3039.0   1109.0  36.492267     296.0   9.740046\n",
      "37     19         El Paso   9956.0   5500.0  55.243070    1683.0  16.904379\n",
      "39     20     San Antonio  16482.0   7128.0  43.247179    2075.0  12.589492\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "xlEcon = pd.read_excel('Data\\8th Grade FY2007 Cohort Workbook.xlsx', sheet_name='TEA Region by Eco', header=None, index_col=None, skiprows=6)\n",
    "\n",
    "#Keep the columns I need\n",
    "xlEcon2=xlEcon[[0,1,2,3,16,17,20,21]]\n",
    "EconTemp=xlEcon2.loc[xlEcon2[2]=='Economically Disadvantaged'].copy()\n",
    "\n",
    "EconTemp2=EconTemp.drop([2], axis=1).copy()\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "Econ=EconTemp2[:20].copy()\n",
    "Econ.columns=['TEAReg','RegName','EcoCoho', 'EconEnr', 'EcopEnr', 'EconComp', 'EcopComp']\n",
    "\n",
    "Econ['EcopEnr']=100*Econ['EcopEnr']\n",
    "Econ['EcopComp']=100*Econ['EcopComp']\n",
    "print(Econ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Here we get overall totals by region for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEAReg         RegName  TotCoho  TotnEnr    TotpEnr  TotnComp   TotpComp\n",
      "0       1        Edinburg  26581.0  15042.0  56.589293    5754.0  21.647041\n",
      "1       2  Corpus Christi   7912.0   3996.0  50.505561    1429.0  18.061173\n",
      "2       3        Victoria   4109.0   2197.0  53.467997     995.0  24.215138\n",
      "3       4         Houston  74398.0  40848.0  54.904702   16498.0  22.175327\n",
      "4       5        Beaumont   6094.0   3224.0  52.904496    1235.0  20.265835\n",
      "..    ...             ...      ...      ...        ...       ...        ...\n",
      "15     16        Amarillo   5830.0   3157.0  54.150943    1278.0  21.921098\n",
      "16     17         Lubbock   5639.0   2855.0  50.629544    1150.0  20.393687\n",
      "17     18         Midland   5880.0   2837.0  48.248299    1021.0  17.363946\n",
      "18     19         El Paso  13214.0   7716.0  58.392614    2605.0  19.713940\n",
      "19     20     San Antonio  27421.0  14194.0  51.763247    5645.0  20.586412\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\8th Grade FY2007 Cohort Workbook.xlsx', sheet_name='Summary', header=None, index_col=None, skiprows=16)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2,15,16,19,20]]\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "RegTotals=xl2[:20].copy()\n",
    "RegTotals.columns=['TEAReg','RegName','TotCoho', 'TotnEnr', 'TotpEnr', 'TotnComp', 'TotpComp']\n",
    "\n",
    "RegTotals['TotpEnr']=100*RegTotals['TotpEnr']\n",
    "RegTotals['TotpComp']=100*RegTotals['TotpComp']\n",
    "\n",
    "print(RegTotals)\n",
    "#RegTotals.to_csv('RegTotals.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now get statewide Cohort totals for Hispanics and African Americans\n",
    "(not used for the maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Cohort\n",
      "Eth                       \n",
      "African American   50684.0\n",
      "Hispanic          149548.0\n",
      "Others             11896.0\n",
      "White             126214.0\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\8th Grade FY2007 Cohort Workbook.xlsx', sheet_name='Summary', header=None, index_col=None, skiprows=38)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2]]\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "StatewideCohortTotals=xl2[:8]\n",
    "StatewideCohortTotals.columns=['Gender','Eth','Cohort']\n",
    "\n",
    "#Get African American and Hispanic Statewide Cohort Totals\n",
    "StatewideCohortTotals=StatewideCohortTotals.groupby([\"Eth\"]).sum().copy()\n",
    "\n",
    "print(StatewideCohortTotals)\n",
    "#StatewideCohortTotals.to_csv('StatewideCohortTotals.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now get statewide Cohort totals for Econ Disadvantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Cohort\n",
      "Eco                                 \n",
      "Economically Disadvantaged  179535.0\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\8th Grade FY2007 Cohort Workbook.xlsx', sheet_name='Summary', header=None, index_col=None, skiprows=52)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2]]\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "StatewideCohortEcon=xl2[:4]\n",
    "StatewideCohortEcon.columns=['Eco','Eth','Cohort']\n",
    "\n",
    "#Get African American and Hispanic Statewide Cohort Totals\n",
    "StatewideCohortEcon=StatewideCohortEcon.groupby([\"Eco\"]).sum().copy()\n",
    "\n",
    "print(StatewideCohortEcon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### And now merge the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f1ec23d46d70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mAll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHisp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEAReg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RegName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mAll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAllmales\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEAReg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RegName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mAll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRegTotals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEAReg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RegName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mAll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEcon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEAReg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RegName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     59\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                          validate=validate)\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# to avoid incompat dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;31m# If argument passed to validate,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    981\u001b[0m             elif ((is_numeric_dtype(lk) and not is_bool_dtype(lk))\n\u001b[0;32m    982\u001b[0m                     and not is_numeric_dtype(rk)):\n\u001b[1;32m--> 983\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m             elif (not is_numeric_dtype(lk)\n\u001b[0;32m    985\u001b[0m                     and (is_numeric_dtype(rk) and not is_bool_dtype(rk))):\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "#Combine into one table\n",
    "All=pd.merge(AA, Hisp,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, Allmales,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, RegTotals,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, Econ,on=['TEAReg', 'RegName']).copy()\n",
    "\n",
    "\n",
    "#Calculate Hisp, AA, and Econ % of statewide cohort for each TEA Region\n",
    "All['AATXCoho']=StatewideCohortTotals.loc['African American','Cohort']\n",
    "All['HisTXCoho']=StatewideCohortTotals.loc['Hispanic','Cohort']\n",
    "All['EcoTXCoho']=StatewideCohortEcon.loc['Economically Disadvantaged','Cohort']\n",
    "All['AApTXCoho']=100*All['AACoho']/All['AATXCoho']\n",
    "All['HispTXCoho']=100*All['HisCoho']/All['HisTXCoho']\n",
    "All['EcopTXCoho']=100*All['EcoCoho']/All['EcoTXCoho']\n",
    "\n",
    "\n",
    "#Calculate % point differences for AA/Hisp/AAmales/Eco enrollmnet and completion rates from total\n",
    "All['AAEnrpDi']=All['AApEnr']-All['TotpEnr']\n",
    "All['HisEnrpDi']=All['HispEnr']-All['TotpEnr']\n",
    "All['AAmEnrpDi']=All['AAmpEnr']-All['TotmpEnr'] #AA males only vs all males (not used)\n",
    "All['TotmEnrpDi']=All['TotmpEnr']-All['TotpEnr'] #all males\n",
    "All['EcoEnrpDi']=All['EcopEnr']-All['TotpEnr']\n",
    "All['AAComppDi']=All['AApComp']-All['TotpComp']\n",
    "All['HisComppDi']=All['HispComp']-All['TotpComp']\n",
    "All['AAmComppDi']=All['AAmpComp']-All['TotmpComp'] #AA males only vs all males (not used)\n",
    "All['TotmCompDi']=All['TotmpComp']-All['TotpComp'] #all males\n",
    "All['EcoComppDi']=All['EcopComp']-All['TotpComp']\n",
    "\n",
    "#Drop unneeded variables\n",
    "Final=All.drop(['HisTXCoho','AATXCoho', 'EcoTXCoho'], axis=1).copy() #Keep the columns I need\n",
    "\n",
    "\n",
    "#Make perc of total for AA, Hisp, Eco, and AA_males\n",
    "Final['AApCoho']=100*All['AACoho']/All['TotCoho']\n",
    "Final['HispCoho']=100*All['HisCoho']/All['TotCoho']\n",
    "Final['AAmpCoho']=100*All['AAmCoho']/All['TotmCoho']\n",
    "Final['EcopCoho']=100*All['EcoCoho']/All['TotCoho']\n",
    "\n",
    "#Make variables with \"_\" suffix. They will have zero decmals and be used as symbol layers\n",
    "Final['TotpEnr_']=Final['TotpEnr']\n",
    "Final['TotpComp_']=Final['TotpComp'] \n",
    "Final['AApCoho_']=Final['AApCoho']\n",
    "Final['AAmpCoho_']=Final['AAmpCoho']\n",
    "Final['HispCoho_']=Final['HispCoho']\n",
    "Final['EcopCoho_']=Final['EcopCoho']\n",
    "Final['AAComppD_']=Final['AAComppDi']\n",
    "Final['AAmComppD_']=Final['AAmComppDi']\n",
    "Final['HisComppD_']=Final['HisComppDi']\n",
    "Final['EcoComppD_']=Final['EcoComppDi']\n",
    "Final['AAEnrpD_']=Final['AAEnrpDi']\n",
    "Final['AAmEnrpD_']=Final['AAmEnrpDi']\n",
    "Final['HisEnrpD_']=Final['HisEnrpDi']\n",
    "Final['EcoEnrpD_']=Final['EcoEnrpDi']\n",
    "Final['TotmEnrpD_']=Final['TotmEnrpDi']\n",
    "Final['TotmCompD_']=Final['TotmCompDi']\n",
    "\n",
    "\n",
    "#set percentages to have just one decimal place\n",
    "Processed = Final.round({'AApEnr': 1, 'AApComp': 1, 'AAmpEnr': 1, 'AAmpComp': 1, \n",
    "             'HispEnr': 1, 'HispComp': 1, 'TotmpEnr': 1, 'TotmpComp': 1, \n",
    "             'TotpEnr': 1, 'TotpComp': 1, 'AApTXCoho': 1, 'AAEnrpDi': 1, \n",
    "             'HisEnrpDi': 1, 'AAmEnrpDi': 1, 'AAComppDi': 1, 'HisComppDi': \n",
    "             1, 'AAmComppDi': 1, 'AApCoho': 1, 'HispCoho': 1, 'AAmpCoho': 1,\n",
    "             'EcopEnr': 1, 'EcopComp': 1, 'EcoEnrpDi': 1, 'EcoComppDi': 1, \n",
    "            'EcopTXCoho': 1,'HispTXCoho': 1, 'EcopCoho':1, 'TotmEnrpDi':1, 'TotmCompDi':1, 'TotpEnr_':0, \n",
    "            'TotpComp_':0, 'AApCoho_':0, 'AAmpCoho_':0, 'HispCoho_':0, 'EcopCoho_':0, \n",
    "            'AAComppD_':0, 'AAmComppD_':0, 'HisComppD_':0, 'EcoComppD_':0,\n",
    "            'AAEnrpD_':0, 'AAmEnrpD_':0, 'HisEnrpD_':0, 'EcoEnrpD_':0, 'TotmEnrpD_':0, 'TotmCompD_':0}).copy()\n",
    "\n",
    "Processed.to_csv('ProcessedData.csv', index=False)\n",
    "print(Processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The rest of the code prepares the shapefiles for mapping.\n",
    "\n",
    "### We'll need:\n",
    "    \n",
    "* Polygons for TEA Regions [available from TEA](http://schoolsdata2-tea-texas.opendata.arcgis.com)\n",
    "* Centroids (points) for TEA Regions\n",
    "* An outline of the State of Texas\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get TEARegion file and unzip\n",
    "URL=requests.get('http://opendata.arcgis.com/datasets/12142ff8beec4a1797334c9c41ba7b18_0.zip')\n",
    "zippedRegions=zipfile.ZipFile(io.BytesIO(URL.content))\n",
    "zippedRegions.extractall('Data/rawESC_Regions')\n",
    "\n",
    "#Delete unnecessary fields\n",
    "arcpy.DeleteField_management(\"Data/rawESC_Regions/ESC_Regions.shp\", ['SHAPE_Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get State of Texas file and unzip\n",
    "URLtexas=requests.get('http://www2.census.gov/geo/tiger/GENZ2016/shp/cb_2016_us_state_5m.zip')\n",
    "zippedState=zipfile.ZipFile(io.BytesIO(URLtexas.content))\n",
    "zippedState.extractall('Data/TexasOutline')\n",
    "\n",
    "#Delete unnecessary fields\n",
    "arcpy.DeleteField_management(\"Data/TexasOutline/cb_2016_us_state_5m.shp\", \n",
    "                             [\"sTATENS\", \"AFFGEOID\", \"STUSPS\", 'NAME', 'LSAD', 'ALAND', 'AWATER'])\n",
    "\n",
    "\n",
    "arcpy.MakeFeatureLayer_management (\"Data/TexasOutline/cb_2016_us_state_5m.shp\", \"TexasOutline\", \"STATEFP='48'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a File Geodatabase and copy shapefile data\n",
    "# uncomment the following line the first time code is run\n",
    "arcpy.CreateFileGDB_management('Data',\"Cohort.gdb\")\n",
    "\n",
    "arcpy.FeatureClassToGeodatabase_conversion('Data/rawESC_Regions/ESC_Regions.shp', 'Data/Cohort.gdb')\n",
    "\n",
    "#List fields in dataset\n",
    "fields = arcpy.ListFields('Data/Cohort.gdb/ESC_Regions')\n",
    "\n",
    "for field in fields:\n",
    "    print(\"{0} is a type of {1} with a length of {2}\"\n",
    "          .format(field.name, field.type, field.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Cohort data to GeoDataBase\n",
    "arcpy.TableToTable_conversion('ProcessedData.csv', 'Data/Cohort.gdb', 'CohortData')\n",
    "\n",
    "#Merge Cohort Data to TEA Region Polygons\n",
    "arcpy.JoinField_management('Data/Cohort.gdb/ESC_Regions', 'OBJECTID','Data/Cohort.gdb/CohortData', 'TEAReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make folder if it doesn't exist\n",
    "if not os.path.exists('Data/FinalShapefiles'):\n",
    "    os.makedirs('Data/FinalShapefiles')\n",
    "    \n",
    "#Export merged TEARegions with Cohort data to shapefile\n",
    "arcpy.FeatureClassToShapefile_conversion ('Data/Cohort.gdb/ESC_Regions', 'Data/FinalShapefiles')\n",
    "\n",
    "#Export TexasOutline to shapefile\n",
    "arcpy.FeatureClassToShapefile_conversion ('TexasOutline', 'Data/FinalShapefiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now make the centrids for the TEA Regions\n",
    "\n",
    "(Requires the advanced license)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set local variables\n",
    "inFeatures = \"Data/Cohort.gdb/ESC_Regions\"\n",
    "outFeatureClass = \"Data/Cohort.gdb/ESC_Points\"\n",
    "\n",
    "# Use FeatureToPoint function to find a point inside each park\n",
    "arcpy.FeatureToPoint_management(inFeatures, outFeatureClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export merged TEARegion Points to shapefile\n",
    "arcpy.FeatureClassToShapefile_conversion ('Data/Cohort.gdb/ESC_Points', 'Data/FinalShapefiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, go to linux and use the GDAL to convert shapefiles to geojson. Then use the Tippecanoe tool to make .MBtiles\n",
    "\n",
    "I used the following commands:\n",
    "\n",
    "* ogr2ogr -f GeoJSON CohortTEARegionPolys.json Data/FinalShapefiles/ESC_Regions.shp -progress\n",
    "* ogr2ogr -f GeoJSON TexasOutline.json Data/FinalShapefiles/TexasOutline.shp -progress\n",
    "* ogr2ogr -f GeoJSON CohortTEARegionPoints.json Data/FinalShapefiles/ESC_Points.shp -progress\n",
    "* tippecanoe --output=8thGradeCohort2007TEARegionData.mbtiles CohortTEARegionPoints.json CohortTEARegionPolys.json TexasOutline.json -r1 --drop-fraction-as-needed  --simplification=9 --maximum-zoom=15 --minimum-zoom=3 --exclude=OBJECTID_1 --detect-shared-borders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
