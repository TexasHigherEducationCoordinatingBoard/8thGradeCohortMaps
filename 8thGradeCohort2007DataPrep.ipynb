{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This program prepares the selected data from the 8th Grade Cohort Longitudinal Study for mapping\n",
    "The 8th grade Cohort Longitudinal Study is a data product of the Texas Higher Education Coordinating Board.\n",
    "\n",
    "### Begin by downloading the 2007 Cohort Workbook from [the THECB website](http://www.txhighereddata.org/index.cfm?objectId=F2CBE4A0-C90B-11E5-8D610050560100A9). \n",
    "\n",
    "The selected data focus on enrollment and completion rates in higher education. In addittion to examining the overall cohort, the data also describe the target populations - African American, and Hispanic, economically disadvantaged, and male students -  from the Texas Higher Education Strategic Plan. The target populations examined here are .\n",
    "\n",
    "### Statistics are presented for each of the 20 regions defined by the Texas Education Agency. These are also know as the Education Service Center (ERC) Regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import arcpy\n",
    "import io\n",
    "import os\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by downloading the 2007 8th grade cohort workbook from the THECB website\n",
    "\n",
    "The cohort workbooks are available at: http://www.txhighereddata.org/index.cfm?objectId=F2CBE4A0-C90B-11E5-8D610050560100A9\n",
    "\n",
    "Save the workbook in 'Data\\8th Grade FY2007 Cohort Workbook.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data\\\\Cohort.gdb'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We'll also make a file geodatabase in the new folder\n",
    "arcpy.CreateFileGDB_management(\"Data\",\"Cohort.gdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll extract data from the workbook starting with the Gender by Ethnicity Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TEAReg      RegName  Gender               Eth    CohoN    nEnr      pEnr  \\\n",
      "0        1     Edinburg  Female  African American     33.0    19.0  0.575758   \n",
      "1        1     Edinburg  Female          Hispanic  12607.0  7559.0  0.599588   \n",
      "2        1     Edinburg  Female             White    383.0   285.0  0.744125   \n",
      "3        1     Edinburg  Female            Others     70.0    44.0  0.628571   \n",
      "4        1     Edinburg    Male  African American     27.0    16.0  0.592593   \n",
      "..     ...          ...     ...               ...      ...     ...       ...   \n",
      "155     20  San Antonio  Female            Others    220.0   160.0  0.727273   \n",
      "156     20  San Antonio    Male  African American   1070.0   513.0  0.479439   \n",
      "157     20  San Antonio    Male          Hispanic   9266.0  4004.0  0.432117   \n",
      "158     20  San Antonio    Male             White   3583.0  2014.0  0.562099   \n",
      "159     20  San Antonio    Male            Others    269.0   180.0  0.669145   \n",
      "\n",
      "      nComp     pComp  \n",
      "0       8.0  0.242424  \n",
      "1    3143.0  0.249306  \n",
      "2     157.0  0.409922  \n",
      "3      35.0  0.500000  \n",
      "4       4.0  0.148148  \n",
      "..      ...       ...  \n",
      "155    99.0  0.450000  \n",
      "156   118.0  0.110280  \n",
      "157  1214.0  0.131017  \n",
      "158   984.0  0.274630  \n",
      "159    91.0  0.338290  \n",
      "\n",
      "[160 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\8th Grade FY2007 Cohort Workbook.xlsx', sheet_name='TEA by Gender by Ethnicity', header=None, index_col=None, skiprows=6)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2,3,4,17,18,21,22]]\n",
    "\n",
    "#Drop the rows I don't need\n",
    "GenEth=xl2[:160]\n",
    "GenEth.columns=['TEAReg','RegName','Gender','Eth', 'CohoN', 'nEnr', 'pEnr', 'nComp', 'pComp']\n",
    "\n",
    "#Make Dataset just of African American Males (60x30TX target popultation - this wasn't used for the maps)\n",
    "AAmales=GenEth.loc[(GenEth['Eth']=='African American') & (GenEth['Gender']=='Male')].copy() #copy to avoid chained indexing\n",
    "AAmales=AAmales.drop(['Gender','Eth'], axis=1)\n",
    "AAmales.columns=['TEAReg','RegName','AAmCoho', 'AAmnEnr', 'AAmpEnr', 'AAmnComp', 'AAmpComp']\n",
    "AAmales['AAmpEnr']=100*AAmales['AAmpEnr']\n",
    "AAmales['AAmpComp']=100*AAmales['AAmpComp']\n",
    "#AAmales.to_csv('AAmales.csv', index=False)\n",
    "\n",
    "print(GenEth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now get African American and Hispanic totals by region. \n",
    "We'll collapse on ethicity to remove gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TEAReg         RegName   AACoho  AAnEnr  AAnComp     AApEnr    AApComp\n",
      "0        1        Edinburg     60.0    35.0     12.0  58.333333  20.000000\n",
      "1        2  Corpus Christi    306.0   151.0     40.0  49.346405  13.071895\n",
      "2        3        Victoria    447.0   222.0     71.0  49.664430  15.883669\n",
      "3        4         Houston  17218.0  9734.0   2546.0  56.533860  14.786851\n",
      "4        5        Beaumont   1813.0   957.0    204.0  52.785438  11.252068\n",
      "..     ...             ...      ...     ...      ...        ...        ...\n",
      "15      16        Amarillo    364.0   184.0     38.0  50.549451  10.439560\n",
      "16      17         Lubbock    480.0   202.0     31.0  42.083333   6.458333\n",
      "17      18         Midland    343.0   143.0     39.0  41.690962  11.370262\n",
      "18      19         El Paso    380.0   170.0     42.0  44.736842  11.052632\n",
      "19      20     San Antonio   2074.0  1095.0    342.0  52.796528  16.489875\n",
      "\n",
      "[20 rows x 7 columns]\n",
      "    TEAReg         RegName  HisCoho  HisnEnr  HisnComp    HispEnr   HispComp\n",
      "0        1        Edinburg  25584.0  14376.0    5405.0  56.191370  21.126485\n",
      "1        2  Corpus Christi   5423.0   2557.0     792.0  47.151023  14.604462\n",
      "2        3        Victoria   1821.0    736.0     229.0  40.417353  12.575508\n",
      "3        4         Houston  30026.0  13183.0    4378.0  43.905282  14.580697\n",
      "4        5        Beaumont    652.0    270.0     105.0  41.411043  16.104294\n",
      "..     ...             ...      ...      ...       ...        ...        ...\n",
      "15      16        Amarillo   2213.0    987.0     350.0  44.600090  15.815635\n",
      "16      17         Lubbock   2819.0   1145.0     351.0  40.617240  12.451224\n",
      "17      18         Midland   3393.0   1429.0     424.0  42.116121  12.496316\n",
      "18      19         El Paso  11690.0   6927.0    2290.0  59.255774  19.589393\n",
      "19      20     San Antonio  18037.0   8618.0    2896.0  47.779564  16.055885\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Keep Hispanic and African American counts, collapse to remove gender, and then recalculate percents \n",
    "EthCounts=GenEth.drop(GenEth.columns[[2,6,8]], axis=1) #axis=0 for rows, axis=1 for columns\n",
    "\n",
    "#Make African American Group\n",
    "AAtemp=EthCounts.loc[EthCounts['Eth']=='African American'].copy() #copy to avoid chained indexing\n",
    "AA=AAtemp.groupby([\"TEAReg\", \"RegName\",\"Eth\"], as_index=False).sum()\n",
    "AA['AApEnr']=100*AA['nEnr']/AA['CohoN']\n",
    "AA['AApComp']=100*AA['nComp']/AA['CohoN']\n",
    "AA=AA.drop(['Eth'], axis=1) #Keep the columns I need\n",
    "AA.columns=['TEAReg','RegName','AACoho', 'AAnEnr','AAnComp','AApEnr','AApComp']\n",
    "\n",
    "#Make Hispanic Group\n",
    "Hisptemp=EthCounts.loc[EthCounts['Eth']=='Hispanic'].copy() #copy to avoid chained indexing\n",
    "Hisp=Hisptemp.groupby([\"TEAReg\", \"RegName\",\"Eth\"], as_index=False).sum()\n",
    "Hisp['HispEnr']=100*Hisp['nEnr']/Hisp['CohoN']\n",
    "Hisp['HispComp']=100*Hisp['nComp']/Hisp['CohoN']\n",
    "Hisp=Hisp.drop(['Eth'], axis=1) #Keep the columns I need\n",
    "Hisp.columns=['TEAReg','RegName','HisCoho', 'HisnEnr','HisnComp','HispEnr','HispComp']\n",
    "\n",
    "print(AA)\n",
    "print(Hisp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Male enrollment and completion by region. \n",
    "For this, we'll collapse on gender and remove ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TEAReg         RegName  TotmCoho  TotmnEnr  TotmnComp   TotmpEnr  \\\n",
      "0        1        Edinburg   13488.0    7135.0     2411.0  52.898873   \n",
      "1        2  Corpus Christi    4113.0    1857.0      601.0  45.149526   \n",
      "2        3        Victoria    2144.0    1008.0      435.0  47.014925   \n",
      "3        4         Houston   37900.0   19325.0     6852.0  50.989446   \n",
      "4        5        Beaumont    3199.0    1503.0      525.0  46.983432   \n",
      "..     ...             ...       ...       ...        ...        ...   \n",
      "15      16        Amarillo    2987.0    1443.0      515.0  48.309340   \n",
      "16      17         Lubbock    2899.0    1361.0      498.0  46.947223   \n",
      "17      18         Midland    3005.0    1311.0      433.0  43.627288   \n",
      "18      19         El Paso    6775.0    3673.0     1041.0  54.214022   \n",
      "19      20     San Antonio   14188.0    6711.0     2407.0  47.300536   \n",
      "\n",
      "    TotmpComp  \n",
      "0   17.875148  \n",
      "1   14.612205  \n",
      "2   20.289179  \n",
      "3   18.079156  \n",
      "4   16.411379  \n",
      "..        ...  \n",
      "15  17.241379  \n",
      "16  17.178337  \n",
      "17  14.409318  \n",
      "18  15.365314  \n",
      "19  16.965041  \n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Get total male counts by region, collape on gender, counts only.\n",
    "GenCounts=GenEth.drop(GenEth.columns[[3,6,8]], axis=1) #axis=0 for rows, axis=1 for columns\n",
    "Allmalestemp=GenCounts.loc[GenCounts['Gender']=='Male'].copy() #copy to avoid chained indexing\n",
    "Allmales=Allmalestemp.groupby([\"TEAReg\", \"RegName\"], as_index=False).sum().copy()\n",
    "Allmales['AllmpEnr']=100*Allmales['nEnr']/Allmales['CohoN']\n",
    "Allmales['AllmpComp']=100*Allmales['nComp']/Allmales['CohoN']\n",
    "Allmales.columns=['TEAReg', 'RegName','TotmCoho', 'TotmnEnr','TotmnComp','TotmpEnr','TotmpComp']\n",
    "\n",
    "print(Allmales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Economic Disadvantaged student data by region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEAReg         RegName  EcoCoho  EconEnr    EcopEnr  EconComp   EcopComp\n",
      "1       1        Edinburg  22752.0  12222.0  53.718354    4377.0  19.237869\n",
      "3       2  Corpus Christi   4635.0   1816.0  39.180151     460.0   9.924488\n",
      "5       3        Victoria   2153.0    833.0  38.690200     250.0  11.611705\n",
      "7       4         Houston  37985.0  16583.0  43.656707    4718.0  12.420692\n",
      "9       5        Beaumont   3056.0   1260.0  41.230366     304.0   9.947644\n",
      "..    ...             ...      ...      ...        ...       ...        ...\n",
      "31     16        Amarillo   3005.0   1280.0  42.595674     393.0  13.078203\n",
      "33     17         Lubbock   3198.0   1211.0  37.867417     329.0  10.287680\n",
      "35     18         Midland   3039.0   1109.0  36.492267     296.0   9.740046\n",
      "37     19         El Paso   9956.0   5500.0  55.243070    1683.0  16.904379\n",
      "39     20     San Antonio  16482.0   7128.0  43.247179    2075.0  12.589492\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "xlEcon = pd.read_excel('Data\\8th Grade FY2007 Cohort Workbook.xlsx', sheet_name='TEA Region by Eco', header=None, index_col=None, skiprows=6)\n",
    "\n",
    "#Keep the columns I need\n",
    "xlEcon2=xlEcon[[0,1,2,3,16,17,20,21]]\n",
    "EconTemp=xlEcon2.loc[xlEcon2[2]=='Economically Disadvantaged'].copy()\n",
    "\n",
    "EconTemp2=EconTemp.drop([2], axis=1).copy()\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "Econ=EconTemp2[:20].copy()\n",
    "Econ.columns=['TEAReg','RegName','EcoCoho', 'EconEnr', 'EcopEnr', 'EconComp', 'EcopComp']\n",
    "\n",
    "Econ['EcopEnr']=100*Econ['EcopEnr']\n",
    "Econ['EcopComp']=100*Econ['EcopComp']\n",
    "print(Econ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Here we get overall totals by region for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEAReg         RegName  TotCoho  TotnEnr    TotpEnr  TotnComp   TotpComp\n",
      "0       1        Edinburg  26581.0  15042.0  56.589293    5754.0  21.647041\n",
      "1       2  Corpus Christi   7912.0   3996.0  50.505561    1429.0  18.061173\n",
      "2       3        Victoria   4109.0   2197.0  53.467997     995.0  24.215138\n",
      "3       4         Houston  74398.0  40848.0  54.904702   16498.0  22.175327\n",
      "4       5        Beaumont   6094.0   3224.0  52.904496    1235.0  20.265835\n",
      "..    ...             ...      ...      ...        ...       ...        ...\n",
      "15     16        Amarillo   5830.0   3157.0  54.150943    1278.0  21.921098\n",
      "16     17         Lubbock   5639.0   2855.0  50.629544    1150.0  20.393687\n",
      "17     18         Midland   5880.0   2837.0  48.248299    1021.0  17.363946\n",
      "18     19         El Paso  13214.0   7716.0  58.392614    2605.0  19.713940\n",
      "19     20     San Antonio  27421.0  14194.0  51.763247    5645.0  20.586412\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\8th Grade FY2007 Cohort Workbook.xlsx', sheet_name='Summary', header=None, index_col=None, skiprows=16)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2,15,16,19,20]]\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "RegTotals=xl2[:20].copy()\n",
    "RegTotals.columns=['TEAReg','RegName','TotCoho', 'TotnEnr', 'TotpEnr', 'TotnComp', 'TotpComp']\n",
    "\n",
    "RegTotals['TotpEnr']=100*RegTotals['TotpEnr']\n",
    "RegTotals['TotpComp']=100*RegTotals['TotpComp']\n",
    "\n",
    "print(RegTotals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### And now merge the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEAReg         RegName   AACoho  AAnEnr  AAnComp  AApEnr  AApComp  HisCoho  \\\n",
      "0       1        Edinburg     60.0    35.0     12.0    58.3     20.0  25584.0   \n",
      "1       2  Corpus Christi    306.0   151.0     40.0    49.3     13.1   5423.0   \n",
      "2       3        Victoria    447.0   222.0     71.0    49.7     15.9   1821.0   \n",
      "3       4         Houston  17218.0  9734.0   2546.0    56.5     14.8  30026.0   \n",
      "4       5        Beaumont   1813.0   957.0    204.0    52.8     11.3    652.0   \n",
      "..    ...             ...      ...     ...      ...     ...      ...      ...   \n",
      "15     16        Amarillo    364.0   184.0     38.0    50.5     10.4   2213.0   \n",
      "16     17         Lubbock    480.0   202.0     31.0    42.1      6.5   2819.0   \n",
      "17     18         Midland    343.0   143.0     39.0    41.7     11.4   3393.0   \n",
      "18     19         El Paso    380.0   170.0     42.0    44.7     11.1  11690.0   \n",
      "19     20     San Antonio   2074.0  1095.0    342.0    52.8     16.5  18037.0   \n",
      "\n",
      "    HisnEnr  HisnComp    ...     HispCoho_  EcopCoho_  AAComppD_  HisComppD_  \\\n",
      "0   14376.0    5405.0    ...          96.0       86.0       -2.0        -1.0   \n",
      "1    2557.0     792.0    ...          69.0       59.0       -5.0        -3.0   \n",
      "2     736.0     229.0    ...          44.0       52.0       -8.0       -12.0   \n",
      "3   13183.0    4378.0    ...          40.0       51.0       -7.0        -8.0   \n",
      "4     270.0     105.0    ...          11.0       50.0       -9.0        -4.0   \n",
      "..      ...       ...    ...           ...        ...        ...         ...   \n",
      "15    987.0     350.0    ...          38.0       52.0      -11.0        -6.0   \n",
      "16   1145.0     351.0    ...          50.0       57.0      -14.0        -8.0   \n",
      "17   1429.0     424.0    ...          58.0       52.0       -6.0        -5.0   \n",
      "18   6927.0    2290.0    ...          88.0       75.0       -9.0        -0.0   \n",
      "19   8618.0    2896.0    ...          66.0       60.0       -4.0        -5.0   \n",
      "\n",
      "    EcoComppD_  AAEnrpD_  HisEnrpD_  EcoEnrpD_  MaleEnrpD_  MaleCpD_  \n",
      "0         -2.0       2.0       -0.0       -3.0        -4.0      -4.0  \n",
      "1         -8.0      -1.0       -3.0      -11.0        -5.0      -3.0  \n",
      "2        -13.0      -4.0      -13.0      -15.0        -6.0      -4.0  \n",
      "3        -10.0       2.0      -11.0      -11.0        -4.0      -4.0  \n",
      "4        -10.0      -0.0      -11.0      -12.0        -6.0      -4.0  \n",
      "..         ...       ...        ...        ...         ...       ...  \n",
      "15        -9.0      -4.0      -10.0      -12.0        -6.0      -5.0  \n",
      "16       -10.0      -9.0      -10.0      -13.0        -4.0      -3.0  \n",
      "17        -8.0      -7.0       -6.0      -12.0        -5.0      -3.0  \n",
      "18        -3.0     -14.0        1.0       -3.0        -4.0      -4.0  \n",
      "19        -8.0       1.0       -4.0       -9.0        -4.0      -4.0  \n",
      "\n",
      "[20 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combine into one table\n",
    "All=pd.merge(AA, Hisp,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, Allmales,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, RegTotals,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, Econ,on=['TEAReg', 'RegName']).copy()\n",
    "\n",
    "#Calculate % point differences for AA/Hisp/Males/Eco enrollmnet and completion rates from total cohort by region\n",
    "All['AAEnrpDi']=All['AApEnr']-All['TotpEnr']\n",
    "All['HisEnrpDi']=All['HispEnr']-All['TotpEnr']\n",
    "All['MaleEnrpDi']=All['TotmpEnr']-All['TotpEnr'] #all males\n",
    "All['EcoEnrpDi']=All['EcopEnr']-All['TotpEnr']\n",
    "All['AAComppDi']=All['AApComp']-All['TotpComp']\n",
    "All['HisComppDi']=All['HispComp']-All['TotpComp']\n",
    "All['MaleCpDi']=All['TotmpComp']-All['TotpComp'] #all males\n",
    "All['EcoComppDi']=All['EcopComp']-All['TotpComp']\n",
    "\n",
    "Final=All\n",
    "\n",
    "#Make perc of total for AA, Hisp, and Eco\n",
    "Final['AApCoho']=100*All['AACoho']/All['TotCoho']\n",
    "Final['HispCoho']=100*All['HisCoho']/All['TotCoho']\n",
    "Final['EcopCoho']=100*All['EcoCoho']/All['TotCoho']\n",
    "\n",
    "#Make variables with \"_\" suffix. They will have zero decmals and be used as symbol layers\n",
    "Final['TotpEnr_']=Final['TotpEnr']\n",
    "Final['TotpComp_']=Final['TotpComp'] \n",
    "Final['TotmpComp_']=Final['TotmpComp']\n",
    "Final['AApComp_']=Final['AApComp']\n",
    "Final['HispComp_']=Final['HispComp']\n",
    "Final['EcopComp_']=Final['EcopComp']\n",
    "\n",
    "\n",
    "Final['AApCoho_']=Final['AApCoho']\n",
    "Final['HispCoho_']=Final['HispCoho']\n",
    "Final['EcopCoho_']=Final['EcopCoho']\n",
    "Final['AAComppD_']=Final['AAComppDi']\n",
    "Final['HisComppD_']=Final['HisComppDi']\n",
    "Final['EcoComppD_']=Final['EcoComppDi']\n",
    "Final['AAEnrpD_']=Final['AAEnrpDi']\n",
    "Final['HisEnrpD_']=Final['HisEnrpDi']\n",
    "Final['EcoEnrpD_']=Final['EcoEnrpDi']\n",
    "Final['MaleEnrpD_']=Final['MaleEnrpDi']\n",
    "Final['MaleCpD_']=Final['MaleCpDi']\n",
    "\n",
    "\n",
    "#set percentages to have just one decimal place\n",
    "Processed = Final.round({'AApEnr': 1, 'AApComp': 1, \n",
    "             'HispEnr': 1, 'HispComp': 1, \n",
    "             'TotmpEnr': 1, 'TotmpComp': 1, \n",
    "             'TotpEnr': 1, 'TotpComp': 1, \n",
    "            'AAEnrpDi': 1,  'AAComppDi': 1,\n",
    "            'HisEnrpDi': 1, 'HisComppDi': 1, \n",
    "             'AApCoho': 1, 'HispCoho': 1, 'EcopCoho':1,  \n",
    "             'EcopEnr': 1, 'EcopComp': 1, \n",
    "            'EcoEnrpDi': 1, 'EcoComppDi': 1, \n",
    "            'MaleEnrpDi':1, 'MaleCpDi':1,\n",
    "            'TotpEnr_':0, 'TotpComp_':0, 'TotmpComp_': 0,\n",
    "            'AApComp_': 0, 'HispComp_': 0, 'EcopComp_': 0, \n",
    "            'AApCoho_':0, 'HispCoho_':0, 'EcopCoho_':0, \n",
    "            'AAComppD_':0, 'HisComppD_':0, 'EcoComppD_':0, 'MaleCpD_':0,\n",
    "            'AAEnrpD_':0, 'HisEnrpD_':0, 'EcoEnrpD_':0, 'MaleEnrpD_':0}).copy()\n",
    "\n",
    "Processed.to_csv('Data/ProcessedData.csv', index=False)\n",
    "print(Processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The rest of the code prepares the shapefiles for mapping.\n",
    "\n",
    "### We'll need:\n",
    "    \n",
    "* Polygons for TEA Regions [available from TEA](http://schoolsdata2-tea-texas.opendata.arcgis.com)\n",
    "* Centroids (points) for TEA Regions\n",
    "* An outline of the State of Texas\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get TEARegion file and unzip\n",
    "URL=requests.get('http://opendata.arcgis.com/datasets/12142ff8beec4a1797334c9c41ba7b18_0.zip')\n",
    "zippedRegions=zipfile.ZipFile(io.BytesIO(URL.content))\n",
    "zippedRegions.extractall('Data/rawESC_Regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'TexasOutline'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get State of Texas file and unzip\n",
    "URLtexas=requests.get('http://www2.census.gov/geo/tiger/GENZ2016/shp/cb_2016_us_state_5m.zip')\n",
    "zippedState=zipfile.ZipFile(io.BytesIO(URLtexas.content))\n",
    "zippedState.extractall('Data/TexasOutline')\n",
    "\n",
    "#Delete unnecessary fields\n",
    "arcpy.DeleteField_management(\"Data/TexasOutline/cb_2016_us_state_5m.shp\", \n",
    "                             [\"sTATENS\", \"AFFGEOID\", \"STUSPS\", 'NAME', 'LSAD', 'ALAND', 'AWATER'])\n",
    "\n",
    "arcpy.MakeFeatureLayer_management (\"Data/TexasOutline/cb_2016_us_state_5m.shp\", \"TexasOutline\", \"STATEFP='48'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTID_1 is a type of OID with a length of 4\n",
      "Shape is a type of Geometry with a length of 0\n",
      "FID_1 is a type of Integer with a length of 4\n",
      "OBJECTID is a type of Integer with a length of 4\n",
      "CITY is a type of String with a length of 80\n",
      "REGION is a type of String with a length of 80\n",
      "ORG_E_ID is a type of Integer with a length of 4\n",
      "WEBSITE is a type of String with a length of 80\n",
      "SHAPE_Leng is a type of Double with a length of 8\n",
      "Shape_Length is a type of Double with a length of 8\n",
      "Shape_Area is a type of Double with a length of 8\n"
     ]
    }
   ],
   "source": [
    "#copy shapefiles to geodatabase\n",
    "arcpy.FeatureClassToGeodatabase_conversion('Data/rawESC_Regions/ESC_Regions.shp', 'Data/Cohort.gdb')\n",
    "\n",
    "\n",
    "#List fields in dataset\n",
    "fields = arcpy.ListFields('Data/Cohort.gdb/ESC_Regions')\n",
    "\n",
    "for field in fields:\n",
    "    print(\"{0} is a type of {1} with a length of {2}\"\n",
    "          .format(field.name, field.type, field.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data/Cohort.gdb/ESC_Regions'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete unnecessary fields\n",
    "arcpy.DeleteField_management(\"Data/Cohort.gdb/ESC_Regions\", [\"FID_1\", \"OBJECTID\", \"CITY\", 'REGION', 'ORG_E_ID', 'WEBSITE', 'SHAPE_Leng'])                            \n",
    "\n",
    "#Add Cohort data to GeoDataBase\n",
    "arcpy.TableToTable_conversion('Data/ProcessedData.csv', 'Data/Cohort.gdb', 'CohortData')\n",
    "\n",
    "#Merge Cohort Data to TEA Region Polygons\n",
    "arcpy.JoinField_management('Data/Cohort.gdb/ESC_Regions', 'OBJECTID_1','Data/Cohort.gdb/CohortData', 'TEAReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data\\\\FinalShapefiles'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make folder if it doesn't exist\n",
    "if not os.path.exists('Data/FinalShapefiles'):\n",
    "    os.makedirs('Data/FinalShapefiles')\n",
    "    \n",
    "#Export merged TEARegions with Cohort data to shapefile\n",
    "arcpy.FeatureClassToShapefile_conversion ('Data/Cohort.gdb/ESC_Regions', 'Data/FinalShapefiles')\n",
    "\n",
    "#Export TexasOutline to shapefile\n",
    "arcpy.FeatureClassToShapefile_conversion ('TexasOutline', 'Data/FinalShapefiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now make the centrids for the TEA Regions\n",
    "\n",
    "(Requires the advanced license)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data\\\\Cohort.gdb\\\\ESC_Points'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Set local variables\n",
    "inFeatures = \"Data/Cohort.gdb/ESC_Regions\"\n",
    "outFeatureClass = \"Data/Cohort.gdb/ESC_Points\"\n",
    "\n",
    "# Use FeatureToPoint function to find a point inside each park\n",
    "arcpy.FeatureToPoint_management(inFeatures, outFeatureClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data\\\\FinalShapefiles'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Export merged TEARegion Points to shapefile\n",
    "arcpy.FeatureClassToShapefile_conversion ('Data/Cohort.gdb/ESC_Points', 'Data/FinalShapefiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, go to linux and use the [GDAL](https://www.gdal.org/) ogr2ogr tool to convert shapefiles to geojson. Then use the [Tippecanoe](https://github.com/mapbox/tippecanoe) tool to make .MBtiles\n",
    "\n",
    "I used the following commands:\n",
    "\n",
    "* ogr2ogr -f GeoJSON CohortTEARegionPolys.json Data/FinalShapefiles/ESC_Regions.shp -progress\n",
    "* ogr2ogr -f GeoJSON TexasOutline.json Data/FinalShapefiles/TexasOutline.shp -progress\n",
    "* ogr2ogr -f GeoJSON CohortTEARegionPoints.json Data/FinalShapefiles/ESC_Points.shp -progress\n",
    "* tippecanoe --output=8thGradeCohort2007TEARegionData.mbtiles CohortTEARegionPoints.json CohortTEARegionPolys.json TexasOutline.json -r1 --drop-fraction-as-needed  --simplification=9 --maximum-zoom=15 --minimum-zoom=3 --exclude=OBJECTID_1 --detect-shared-borders\n",
    "\n",
    "Finally, we uploaded the custom .MBtiles to mapbox studio and served them from there. You could also set up your own vector tile server using TileServer-GL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
