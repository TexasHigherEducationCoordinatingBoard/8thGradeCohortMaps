{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This program prepares the selected data from 2006 8th Grade Cohort Longitudinal Study for mapping\n",
    "\n",
    "### Begin by downloading the Cohort Workbook from [the THECB website](http://www.txhighereddata.org/index.cfm?objectId=F2CBE4A0-C90B-11E5-8D610050560100A9). \n",
    "\n",
    "The selected data focuses on target populations from the Texas Higher Education Strategic Plan. The target populations examined here are African American students, particularly African american male students, and Hispanic students.\n",
    "\n",
    "### Target populations are examined by TEA Region which is how the data is presented by the THECB cohort workbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\John\\\\Dropbox\\\\MapDev\\\\Eighth Grade Cohort Map'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import arcpy\n",
    "import io\n",
    "import os\n",
    "pd.options.display.max_rows = 10\n",
    "os.chdir('C:\\\\Users\\John\\Dropbox\\MapDev\\Eighth Grade Cohort Map')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by downloading the 2006 cohort workbook from the THECB website\n",
    "\n",
    "The cohort workbooks are available at: http://www.txhighereddata.org/index.cfm?objectId=F2CBE4A0-C90B-11E5-8D610050560100A9\n",
    "\n",
    "Save the workbook in 'Data\\CohortWorkbook2006.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First get the Gender by Ethnicity Data\n",
    "\n",
    "This next part gets us:\n",
    "\n",
    "African American males (a 60x30 target population) by TEA region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TEAReg         RegName  AAmCoho  AAmnEnr    AAmpEnr  AAmnComp   AAmpComp\n",
      "4        1        Edinburg     24.0     13.0  54.166667       4.0  16.666667\n",
      "12       2  Corpus Christi    148.0     63.0  42.567568      15.0  10.135135\n",
      "20       3        Victoria    226.0    106.0  46.902655      19.0   8.407080\n",
      "28       4         Houston   8803.0   4277.0  48.585709     833.0   9.462683\n",
      "36       5        Beaumont    946.0    446.0  47.145877      83.0   8.773784\n",
      "..     ...             ...      ...      ...        ...       ...        ...\n",
      "124     16        Amarillo    152.0     75.0  49.342105      12.0   7.894737\n",
      "132     17         Lubbock    257.0     88.0  34.241245      12.0   4.669261\n",
      "140     18         Midland    156.0     61.0  39.102564      10.0   6.410256\n",
      "148     19         El Paso    197.0     84.0  42.639594      17.0   8.629442\n",
      "156     20     San Antonio   1054.0    500.0  47.438330     120.0  11.385199\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\CohortWorkbook2006.xlsx', sheetname='TEA by Gender by Ethnicity', header=None, index_col=None, skiprows=6)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2,3,4,17,18,21,22]]\n",
    "\n",
    "#Drop the rows I don't need\n",
    "GenEth=xl2[:160]\n",
    "GenEth.columns=['TEAReg','RegName','Gender','Eth', 'CohoN', 'nEnr', 'pEnr', 'nComp', 'pComp']\n",
    "\n",
    "#Make Dataset just of African American Males (60x30TX target popultation)\n",
    "AAmales=GenEth.loc[(GenEth['Eth']=='African American') & (GenEth['Gender']=='Male')].copy() #copy to avoid chained indexing\n",
    "AAmales=AAmales.drop(['Gender','Eth'], axis=1) #Keep the columns I need\n",
    "AAmales.columns=['TEAReg','RegName','AAmCoho', 'AAmnEnr', 'AAmpEnr', 'AAmnComp', 'AAmpComp']\n",
    "AAmales['AAmpEnr']=100*AAmales['AAmpEnr']\n",
    "AAmales['AAmpComp']=100*AAmales['AAmpComp']\n",
    "\n",
    "print(AAmales)\n",
    "#AAmales.to_csv('AAmales.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part gets us:\n",
    "\n",
    "African American and Hispanic totals by region. For this, we'll collapse on ethicity to remove gender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TEAReg         RegName   AACoho  AAnEnr  AAnComp     AApEnr    AApComp\n",
      "0        1        Edinburg     56.0    34.0     10.0  60.714286  17.857143\n",
      "1        2  Corpus Christi    277.0   140.0     32.0  50.541516  11.552347\n",
      "2        3        Victoria    434.0   224.0     54.0  51.612903  12.442396\n",
      "3        4         Houston  17245.0  9436.0   2278.0  54.717309  13.209626\n",
      "4        5        Beaumont   1870.0  1004.0    234.0  53.689840  12.513369\n",
      "..     ...             ...      ...     ...      ...        ...        ...\n",
      "15      16        Amarillo    318.0   176.0     38.0  55.345912  11.949686\n",
      "16      17         Lubbock    494.0   200.0     38.0  40.485830   7.692308\n",
      "17      18         Midland    323.0   135.0     30.0  41.795666   9.287926\n",
      "18      19         El Paso    366.0   171.0     47.0  46.721311  12.841530\n",
      "19      20     San Antonio   2034.0  1023.0    279.0  50.294985  13.716814\n",
      "\n",
      "[20 rows x 7 columns]\n",
      "    TEAReg         RegName  HisCoho  HisnEnr  HisnComp    HispEnr   HispComp\n",
      "0        1        Edinburg  24839.0  13925.0    4999.0  56.061033  20.125609\n",
      "1        2  Corpus Christi   5545.0   2701.0     774.0  48.710550  13.958521\n",
      "2        3        Victoria   1824.0    721.0     237.0  39.528509  12.993421\n",
      "3        4         Houston  28716.0  12344.0    3880.0  42.986488  13.511631\n",
      "4        5        Beaumont    541.0    225.0      95.0  41.589649  17.560074\n",
      "..     ...             ...      ...      ...       ...        ...        ...\n",
      "15      16        Amarillo   2258.0   1067.0     340.0  47.254207  15.057573\n",
      "16      17         Lubbock   2874.0   1155.0     307.0  40.187891  10.681976\n",
      "17      18         Midland   3233.0   1424.0     421.0  44.045778  13.021961\n",
      "18      19         El Paso  11410.0   6586.0    2061.0  57.721297  18.063103\n",
      "19      20     San Antonio  17575.0   8370.0    2573.0  47.624467  14.640114\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Keep Hispanic and African American counts, collapse to remove gender, and then recalculate percents \n",
    "EthCounts=GenEth.drop(GenEth.columns[[2,6,8]], axis=1) #axis=0 for rows, axis=1 for columns\n",
    "#AA_Hisp=EthCounts.loc[EthCounts['Eth'].isin(['African American', 'Hispanic'])]\n",
    "#AA_Hisp_collapsed=AA_Hisp.groupby([\"TEAReg\", \"RegName\",\"Eth\"]).sum()\n",
    "\n",
    "\n",
    "#Make African American Group\n",
    "AAtemp=EthCounts.loc[EthCounts['Eth']=='African American'].copy() #copy to avoid chained indexing\n",
    "AA=AAtemp.groupby([\"TEAReg\", \"RegName\",\"Eth\"], as_index=False).sum()\n",
    "AA['AApEnr']=100*AA['nEnr']/AA['CohoN']\n",
    "AA['AApComp']=100*AA['nComp']/AA['CohoN']\n",
    "AA=AA.drop(['Eth'], axis=1) #Keep the columns I need\n",
    "AA.columns=['TEAReg','RegName','AACoho', 'AAnEnr','AAnComp','AApEnr','AApComp']\n",
    "\n",
    "#Make Hispanic Group\n",
    "Hisptemp=EthCounts.loc[EthCounts['Eth']=='Hispanic'].copy() #copy to avoid chained indexing\n",
    "Hisp=Hisptemp.groupby([\"TEAReg\", \"RegName\",\"Eth\"], as_index=False).sum()\n",
    "Hisp['HispEnr']=100*Hisp['nEnr']/Hisp['CohoN']\n",
    "Hisp['HispComp']=100*Hisp['nComp']/Hisp['CohoN']\n",
    "Hisp=Hisp.drop(['Eth'], axis=1) #Keep the columns I need\n",
    "Hisp.columns=['TEAReg','RegName','HisCoho', 'HisnEnr','HisnComp','HispEnr','HispComp']\n",
    "\n",
    "\n",
    "#print(AA_Hisp_collapsed)\n",
    "print(AA)\n",
    "print(Hisp)\n",
    "#AA.to_csv('AA.csv')\n",
    "#Hisp.to_csv('Hisp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part gets us:\n",
    "\n",
    "All Males by TEA Region for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TEAReg         RegName  TotmCoho  TotmnEnr  TotmnComp   TotmpEnr  \\\n",
      "0        1        Edinburg   13027.0    6924.0     2142.0  53.151148   \n",
      "1        2  Corpus Christi    4177.0    1981.0      563.0  47.426383   \n",
      "2        3        Victoria    2161.0    1024.0      401.0  47.385470   \n",
      "3        4         Houston   38121.0   19462.0     6838.0  51.053225   \n",
      "4        5        Beaumont    3130.0    1516.0      524.0  48.434505   \n",
      "..     ...             ...       ...       ...        ...        ...   \n",
      "15      16        Amarillo    3026.0    1556.0      600.0  51.421018   \n",
      "16      17         Lubbock    2979.0    1403.0      488.0  47.096341   \n",
      "17      18         Midland    2987.0    1313.0      407.0  43.957148   \n",
      "18      19         El Paso    6592.0    3569.0      943.0  54.141383   \n",
      "19      20     San Antonio   13961.0    6735.0     2205.0  48.241530   \n",
      "\n",
      "    TotmpComp  \n",
      "0   16.442773  \n",
      "1   13.478573  \n",
      "2   18.556224  \n",
      "3   17.937620  \n",
      "4   16.741214  \n",
      "..        ...  \n",
      "15  19.828156  \n",
      "16  16.381336  \n",
      "17  13.625711  \n",
      "18  14.305218  \n",
      "19  15.793998  \n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Get total male counts by region, collape on gender, counts only.\n",
    "GenCounts=GenEth.drop(GenEth.columns[[3,6,8]], axis=1) #axis=0 for rows, axis=1 for columns\n",
    "Allmalestemp=GenCounts.loc[GenCounts['Gender']=='Male'].copy() #copy to avoid chained indexing\n",
    "Allmales=Allmalestemp.groupby([\"TEAReg\", \"RegName\"], as_index=False).sum().copy()\n",
    "Allmales['AllmpEnr']=100*Allmales['nEnr']/Allmales['CohoN']\n",
    "Allmales['AApComp']=100*Allmales['nComp']/Allmales['CohoN']\n",
    "Allmales.columns=['TEAReg', 'RegName','TotmCoho', 'TotmnEnr','TotmnComp','TotmpEnr','TotmpComp']\n",
    "\n",
    "\n",
    "print(Allmales)\n",
    "#Allmales.to_csv('Allmales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEAReg         RegName  EcoCoho  EconEnr    EcopEnr  EconComp   EcopComp\n",
      "1       1        Edinburg  22128.0  11836.0  53.488792    4021.0  18.171547\n",
      "3       2  Corpus Christi   4755.0   1973.0  41.493165     433.0   9.106204\n",
      "5       3        Victoria   2209.0    821.0  37.166139     231.0  10.457220\n",
      "7       4         Houston  37986.0  16011.0  42.149739    4282.0  11.272574\n",
      "9       5        Beaumont   3099.0   1356.0  43.756050     362.0  11.681187\n",
      "..    ...             ...      ...      ...        ...       ...        ...\n",
      "31     16        Amarillo   3040.0   1357.0  44.638158     374.0  12.302632\n",
      "33     17         Lubbock   3233.0   1188.0  36.746056     284.0   8.784411\n",
      "35     18         Midland   3013.0   1152.0  38.234318     301.0   9.990043\n",
      "37     19         El Paso   9690.0   5214.0  53.808050    1502.0  15.500516\n",
      "39     20     San Antonio  16670.0   7282.0  43.683263    1947.0  11.679664\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "xlEcon = pd.read_excel('Data\\CohortWorkbook2006.xlsx', sheetname='TEA Region by Eco', header=None, index_col=None, skiprows=6)\n",
    "\n",
    "#Keep the columns I need\n",
    "xlEcon2=xlEcon[[0,1,2,3,16,17,20,21]]\n",
    "EconTemp=xlEcon2.loc[xlEcon2[2]=='Economically Disadvantaged'].copy()\n",
    "\n",
    "EconTemp2=EconTemp.drop([2], axis=1).copy()\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "Econ=EconTemp2[:20].copy()\n",
    "Econ.columns=['TEAReg','RegName','EcoCoho', 'EconEnr', 'EcopEnr', 'EconComp', 'EcopComp']\n",
    "\n",
    "Econ['EcopEnr']=100*Econ['EcopEnr']\n",
    "Econ['EcopComp']=100*Econ['EcopComp']\n",
    "print(Econ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Here we get overall totals by region for comparison\n",
    "\n",
    " 1. All students for comparison to Ethnicity breakouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEAReg         RegName  TotCoho  TotnEnr    TotpEnr  TotnComp   TotpComp\n",
      "0       1        Edinburg  25852.0  14627.0  56.579762    5322.0  20.586415\n",
      "1       2  Corpus Christi   8181.0   4254.0  51.998533    1411.0  17.247280\n",
      "2       3        Victoria   4068.0   2136.0  52.507375     929.0  22.836775\n",
      "3       4         Houston  74103.0  40647.0  54.852030   16107.0  21.735962\n",
      "4       5        Beaumont   6140.0   3332.0  54.267101    1274.0  20.749186\n",
      "..    ...             ...      ...      ...        ...       ...        ...\n",
      "15     16        Amarillo   5944.0   3384.0  56.931359    1405.0  23.637281\n",
      "16     17         Lubbock   5776.0   2936.0  50.831025    1159.0  20.065789\n",
      "17     18         Midland   5856.0   2894.0  49.419399    1031.0  17.605874\n",
      "18     19         El Paso  12980.0   7454.0  57.426810    2405.0  18.528505\n",
      "19     20     San Antonio  27156.0  14189.0  52.249963    5241.0  19.299602\n",
      "\n",
      "[20 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\CohortWorkbook2006.xlsx', sheetname='Summary', header=None, index_col=None, skiprows=16)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2,15,16,19,20]]\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "RegTotals=xl2[:20].copy()\n",
    "RegTotals.columns=['TEAReg','RegName','TotCoho', 'TotnEnr', 'TotpEnr', 'TotnComp', 'TotpComp']\n",
    "\n",
    "RegTotals['TotpEnr']=100*RegTotals['TotpEnr']\n",
    "RegTotals['TotpComp']=100*RegTotals['TotpComp']\n",
    "\n",
    "print(RegTotals)\n",
    "#RegTotals.to_csv('RegTotals.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now get statewide Cohort totals for Hispanics and African Americans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Cohort\n",
      "Eth                       \n",
      "African American   50671.0\n",
      "Hispanic          144200.0\n",
      "Others             11111.0\n",
      "White             129726.0\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\CohortWorkbook2006.xlsx', sheetname='Summary', header=None, index_col=None, skiprows=38)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2]]\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "StatewideCohortTotals=xl2[:8]\n",
    "StatewideCohortTotals.columns=['Gender','Eth','Cohort']\n",
    "\n",
    "#Get African American and Hispanic Statewide Cohort Totals\n",
    "StatewideCohortTotals=StatewideCohortTotals.groupby([\"Eth\"]).sum().copy()\n",
    "\n",
    "print(StatewideCohortTotals)\n",
    "#StatewideCohortTotals.to_csv('StatewideCohortTotals.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now get statewide Cohort totals for Econ Disadvantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Cohort\n",
      "Eco                               \n",
      "Economically Disadvantaged  177058\n"
     ]
    }
   ],
   "source": [
    "xl = pd.read_excel('Data\\CohortWorkbook2006.xlsx', sheetname='Summary', header=None, index_col=None, skiprows=52)\n",
    "\n",
    "#Keep the columns I need\n",
    "xl2=xl[[0,1,2]]\n",
    "\n",
    "#Get Region Totals and drop the rows I don't need\n",
    "StatewideCohortEcon=xl2[:4]\n",
    "StatewideCohortEcon.columns=['Eco','Eth','Cohort']\n",
    "\n",
    "#Get African American and Hispanic Statewide Cohort Totals\n",
    "StatewideCohortEcon=StatewideCohortEcon.groupby([\"Eco\"]).sum().copy()\n",
    "\n",
    "print(StatewideCohortEcon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### And now merge the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEAReg         RegName   AACoho  AAnEnr  AAnComp  AApEnr  AApComp  AAmCoho  \\\n",
      "0       1        Edinburg     56.0    34.0     10.0    60.7     17.9     24.0   \n",
      "1       2  Corpus Christi    277.0   140.0     32.0    50.5     11.6    148.0   \n",
      "2       3        Victoria    434.0   224.0     54.0    51.6     12.4    226.0   \n",
      "3       4         Houston  17245.0  9436.0   2278.0    54.7     13.2   8803.0   \n",
      "4       5        Beaumont   1870.0  1004.0    234.0    53.7     12.5    946.0   \n",
      "..    ...             ...      ...     ...      ...     ...      ...      ...   \n",
      "15     16        Amarillo    318.0   176.0     38.0    55.3     11.9    152.0   \n",
      "16     17         Lubbock    494.0   200.0     38.0    40.5      7.7    257.0   \n",
      "17     18         Midland    323.0   135.0     30.0    41.8      9.3    156.0   \n",
      "18     19         El Paso    366.0   171.0     47.0    46.7     12.8    197.0   \n",
      "19     20     San Antonio   2034.0  1023.0    279.0    50.3     13.7   1054.0   \n",
      "\n",
      "    AAmnEnr  AAmpEnr    ...      HispCoho.  EcopCoho.  AAComppD.  AAmComppD.  \\\n",
      "0      13.0     54.2    ...           96.0       86.0       -3.0         0.0   \n",
      "1      63.0     42.6    ...           68.0       58.0       -6.0        -3.0   \n",
      "2     106.0     46.9    ...           45.0       54.0      -10.0       -10.0   \n",
      "3    4277.0     48.6    ...           39.0       51.0       -9.0        -8.0   \n",
      "4     446.0     47.1    ...            9.0       50.0       -8.0        -8.0   \n",
      "..      ...      ...    ...            ...        ...        ...         ...   \n",
      "15     75.0     49.3    ...           38.0       51.0      -12.0       -12.0   \n",
      "16     88.0     34.2    ...           50.0       56.0      -12.0       -12.0   \n",
      "17     61.0     39.1    ...           55.0       51.0       -8.0        -7.0   \n",
      "18     84.0     42.6    ...           88.0       75.0       -6.0        -6.0   \n",
      "19    500.0     47.4    ...           65.0       61.0       -6.0        -4.0   \n",
      "\n",
      "    HisComppD.  EcoComppD.  AAEnrpD.  AAmEnrpD.  HisEnrpD.  EcoEnrpD.  \n",
      "0         -0.0        -2.0       4.0        1.0       -1.0       -3.0  \n",
      "1         -3.0        -8.0      -1.0       -5.0       -3.0      -11.0  \n",
      "2        -10.0       -12.0      -1.0       -0.0      -13.0      -15.0  \n",
      "3         -8.0       -10.0      -0.0       -2.0      -12.0      -13.0  \n",
      "4         -3.0        -9.0      -1.0       -1.0      -13.0      -11.0  \n",
      "..         ...         ...       ...        ...        ...        ...  \n",
      "15        -9.0       -11.0      -2.0       -2.0      -10.0      -12.0  \n",
      "16        -9.0       -11.0     -10.0      -13.0      -11.0      -14.0  \n",
      "17        -5.0        -8.0      -8.0       -5.0       -5.0      -11.0  \n",
      "18        -0.0        -3.0     -11.0      -12.0        0.0       -4.0  \n",
      "19        -5.0        -8.0      -2.0       -1.0       -5.0       -9.0  \n",
      "\n",
      "[20 rows x 61 columns]\n",
      "   TEAReg         RegName   AACoho  AAnEnr  AAnComp  AApEnr  AApComp  AAmCoho  \\\n",
      "0       1        Edinburg     56.0    34.0     10.0    60.7     17.9     24.0   \n",
      "1       2  Corpus Christi    277.0   140.0     32.0    50.5     11.6    148.0   \n",
      "2       3        Victoria    434.0   224.0     54.0    51.6     12.4    226.0   \n",
      "3       4         Houston  17245.0  9436.0   2278.0    54.7     13.2   8803.0   \n",
      "4       5        Beaumont   1870.0  1004.0    234.0    53.7     12.5    946.0   \n",
      "..    ...             ...      ...     ...      ...     ...      ...      ...   \n",
      "15     16        Amarillo    318.0   176.0     38.0    55.3     11.9    152.0   \n",
      "16     17         Lubbock    494.0   200.0     38.0    40.5      7.7    257.0   \n",
      "17     18         Midland    323.0   135.0     30.0    41.8      9.3    156.0   \n",
      "18     19         El Paso    366.0   171.0     47.0    46.7     12.8    197.0   \n",
      "19     20     San Antonio   2034.0  1023.0    279.0    50.3     13.7   1054.0   \n",
      "\n",
      "    AAmnEnr  AAmpEnr    ...      HispCoho.  EcopCoho.  AAComppD.  AAmComppD.  \\\n",
      "0      13.0     54.2    ...           96.0       86.0       -3.0         0.0   \n",
      "1      63.0     42.6    ...           68.0       58.0       -6.0        -3.0   \n",
      "2     106.0     46.9    ...           45.0       54.0      -10.0       -10.0   \n",
      "3    4277.0     48.6    ...           39.0       51.0       -9.0        -8.0   \n",
      "4     446.0     47.1    ...            9.0       50.0       -8.0        -8.0   \n",
      "..      ...      ...    ...            ...        ...        ...         ...   \n",
      "15     75.0     49.3    ...           38.0       51.0      -12.0       -12.0   \n",
      "16     88.0     34.2    ...           50.0       56.0      -12.0       -12.0   \n",
      "17     61.0     39.1    ...           55.0       51.0       -8.0        -7.0   \n",
      "18     84.0     42.6    ...           88.0       75.0       -6.0        -6.0   \n",
      "19    500.0     47.4    ...           65.0       61.0       -6.0        -4.0   \n",
      "\n",
      "    HisComppD.  EcoComppD.  AAEnrpD.  AAmEnrpD.  HisEnrpD.  EcoEnrpD.  \n",
      "0         -0.0        -2.0       4.0        1.0       -1.0       -3.0  \n",
      "1         -3.0        -8.0      -1.0       -5.0       -3.0      -11.0  \n",
      "2        -10.0       -12.0      -1.0       -0.0      -13.0      -15.0  \n",
      "3         -8.0       -10.0      -0.0       -2.0      -12.0      -13.0  \n",
      "4         -3.0        -9.0      -1.0       -1.0      -13.0      -11.0  \n",
      "..         ...         ...       ...        ...        ...        ...  \n",
      "15        -9.0       -11.0      -2.0       -2.0      -10.0      -12.0  \n",
      "16        -9.0       -11.0     -10.0      -13.0      -11.0      -14.0  \n",
      "17        -5.0        -8.0      -8.0       -5.0       -5.0      -11.0  \n",
      "18        -0.0        -3.0     -11.0      -12.0        0.0       -4.0  \n",
      "19        -5.0        -8.0      -2.0       -1.0       -5.0       -9.0  \n",
      "\n",
      "[20 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "#set percentages to have just one decimal place\n",
    "All=pd.merge(AA, AAmales,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, Hisp,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, Allmales,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, RegTotals,on=['TEAReg', 'RegName']).copy()\n",
    "All=pd.merge(All, Econ,on=['TEAReg', 'RegName']).copy()\n",
    "\n",
    "\n",
    "#Calculate Hisp, AA, and Econ % of statewide cohort for each TEA Region\n",
    "All['AATXCoho']=StatewideCohortTotals.loc['African American','Cohort']\n",
    "All['HisTXCoho']=StatewideCohortTotals.loc['Hispanic','Cohort']\n",
    "All['EcoTXCoho']=StatewideCohortEcon.loc['Economically Disadvantaged','Cohort']\n",
    "All['AApTXCoho']=100*All['AACoho']/All['AATXCoho']\n",
    "All['HispTXCoho']=100*All['HisCoho']/All['HisTXCoho']\n",
    "All['EcopTXCoho']=100*All['EcoCoho']/All['EcoTXCoho']\n",
    "\n",
    "\n",
    "#Calculate % point differences for AA/Hisp/AAmales/Eco enrollmnet and completion rates from total\n",
    "All['AAEnrpDi']=All['AApEnr']-All['TotpEnr']\n",
    "All['HisEnrpDi']=All['HispEnr']-All['TotpEnr']\n",
    "All['AAmEnrpDi']=All['AAmpEnr']-All['TotmpEnr']\n",
    "All['EcoEnrpDi']=All['EcopEnr']-All['TotpEnr']\n",
    "All['AAComppDi']=All['AApComp']-All['TotpComp']\n",
    "All['HisComppDi']=All['HispComp']-All['TotpComp']\n",
    "All['AAmComppDi']=All['AAmpComp']-All['TotmpComp']\n",
    "All['EcoComppDi']=All['EcopComp']-All['TotpComp']\n",
    "\n",
    "#Drop unneeded variables\n",
    "Final=All.drop(['HisTXCoho','AATXCoho', 'EcoTXCoho'], axis=1).copy() #Keep the columns I need\n",
    "\n",
    "\n",
    "#Make perc of total for AA, Hisp, Eco, and AA_males\n",
    "Final['AApCoho']=100*All['AACoho']/All['TotCoho']\n",
    "Final['HispCoho']=100*All['HisCoho']/All['TotCoho']\n",
    "Final['AAmpCoho']=100*All['AAmCoho']/All['TotmCoho']\n",
    "Final['EcopCoho']=100*All['EcoCoho']/All['TotCoho']\n",
    "\n",
    "#Make variables to have zero decmals to use as symbol layers\n",
    "Final['TotpEnr_']=Final['TotpEnr']\n",
    "Final['TotpComp_']=Final['TotpComp'] \n",
    "Final['AApCoho_']=Final['AApCoho']\n",
    "Final['AAmpCoho_']=Final['AAmpCoho']\n",
    "Final['HispCoho_']=Final['HispCoho']\n",
    "Final['EcopCoho_']=Final['EcopCoho']\n",
    "Final['AAComppD_']=Final['AAComppDi']\n",
    "Final['AAmComppD_']=Final['AAmComppDi']\n",
    "Final['HisComppD_']=Final['HisComppDi']\n",
    "Final['EcoComppD_']=Final['EcoComppDi']\n",
    "Final['AAEnrpD_']=Final['AAEnrpDi']\n",
    "Final['AAmEnrpD_']=Final['AAmEnrpDi']\n",
    "Final['HisEnrpD_']=Final['HisEnrpDi']\n",
    "Final['EcoEnrpD_']=Final['EcoEnrpDi']\n",
    "\n",
    "\n",
    "#set percentages to have just one decimal place\n",
    "Processed = Final.round({'AApEnr': 1, 'AApComp': 1, 'AAmpEnr': 1, 'AAmpComp': 1, \n",
    "             'HispEnr': 1, 'HispComp': 1, 'TotmpEnr': 1, 'TotmpComp': 1, \n",
    "             'TotpEnr': 1, 'TotpComp': 1, 'AApTXCoho': 1, 'AAEnrpDi': 1, \n",
    "             'HisEnrpDi': 1, 'AAmEnrpDi': 1, 'AAComppDi': 1, 'HisComppDi': \n",
    "             1, 'AAmComppDi': 1, 'AApCoho': 1, 'HispCoho': 1, 'AAmpCoho': 1,\n",
    "             'EcopEnr': 1, 'EcopComp': 1, 'EcoEnrpDi': 1, 'EcoComppDi': 1, \n",
    "            'EcopTXCoho': 1,'HispTXCoho': 1, 'EcopCoho':1, 'TotpEnr_':0, \n",
    "            'TotpComp_':0, 'AApCoho_':0, 'AAmpCoho_':0, 'HispCoho_':0, 'EcopCoho_':0, \n",
    "            'AAComppD_':0, 'AAmComppD_':0, 'HisComppD_':0, 'EcoComppD_':0,\n",
    "            'AAEnrpD_':0, 'AAmEnrpD_':0, 'HisEnrpD_':0, 'EcoEnrpD_':0}).copy()\n",
    "\n",
    "Processed.to_csv('ProcessedData.csv', index=False)\n",
    "print(Processed)\n",
    "Processed.to_csv('ProcessedData.csv', index=False)\n",
    "print(Processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The rest of the code prepares the shapefiles for mapping.\n",
    "\n",
    "### We'll need:\n",
    "    \n",
    "* Polygons for TEA Regions [available from TEA](http://schoolsdata2-tea-texas.opendata.arcgis.com)\n",
    "* Centroids (points) for TEA Regions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data/rawESC_Regions/ESC_Regions.shp'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get TEARegion file and unzip\n",
    "URL=requests.get('https://opendata.arcgis.com/datasets/12142ff8beec4a1797334c9c41ba7b18_0.zip')\n",
    "zippedRegions=zipfile.ZipFile(io.BytesIO(URL.content))\n",
    "zippedRegions.extractall('Data/rawESC_Regions')\n",
    "\n",
    "#Delete unnecessary fields\n",
    "arcpy.DeleteField_management(\"Data/rawESC_Regions/ESC_Regions.shp\", \n",
    "                             [\"FID_1\", \"OBJECTID\", \"CITY\", 'REGION', 'ORG_E_ID', 'WEBSITE', 'SHAPE_Leng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTID is a type of OID with a length of 4\n",
      "Shape is a type of Geometry with a length of 0\n",
      "Shape_Length is a type of Double with a length of 8\n",
      "Shape_Area is a type of Double with a length of 8\n"
     ]
    }
   ],
   "source": [
    "# Create a File Geodatabase and copy shapefile data\n",
    "# uncomment the following line the first time code is run\n",
    "arcpy.CreateFileGDB_management('Data',\"Cohort.gdb\")\n",
    "\n",
    "arcpy.FeatureClassToGeodatabase_conversion('Data/rawESC_Regions/ESC_Regions.shp', 'Data/Cohort.gdb')\n",
    "\n",
    "#List fields in dataset\n",
    "fields = arcpy.ListFields('Data/Cohort.gdb/ESC_Regions')\n",
    "\n",
    "for field in fields:\n",
    "    print(\"{0} is a type of {1} with a length of {2}\"\n",
    "          .format(field.name, field.type, field.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data/Cohort.gdb/ESC_Regions'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add Cohort data to GeoDataBase\n",
    "arcpy.TableToTable_conversion('ProcessedData.csv', 'Data/Cohort.gdb', 'CohortData')\n",
    "\n",
    "#Merge Cohort Data to TEA Region Polygons\n",
    "arcpy.JoinField_management('Data/Cohort.gdb/ESC_Regions', 'OBJECTID','Data/Cohort.gdb/CohortData', 'TEAReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data\\\\FinalShapefiles'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs('Data/FinalShapefiles')\n",
    "#Export merged TEARegions with Cohort data to shapefile\n",
    "arcpy.FeatureClassToShapefile_conversion ('Data/Cohort.gdb/ESC_Regions', 'Data/FinalShapefiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now make the centrids for the TEA Regions\n",
    "\n",
    "(Requires the advanced license)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data\\\\Cohort.gdb\\\\ESC_Points'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Set local variables\n",
    "inFeatures = \"Data/Cohort.gdb/ESC_Regions\"\n",
    "outFeatureClass = \"Data/Cohort.gdb/ESC_Points\"\n",
    "\n",
    "# Use FeatureToPoint function to find a point inside each park\n",
    "arcpy.FeatureToPoint_management(inFeatures, outFeatureClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result 'Data\\\\FinalShapefiles'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Export merged TEARegion Points to shapefile\n",
    "arcpy.FeatureClassToShapefile_conversion ('Data/Cohort.gdb/ESC_Points', 'Data/FinalShapefiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, go to linux and use the GDAL to convert shapefiles to geojson. Then use the Tippecanoe tool to make .MBtiles\n",
    "\n",
    "I used the following commands:\n",
    "\n",
    "* ogr2ogr -f GeoJSON Cohort2006TEARegionPolys.json Data/FinalShapefiles/ESC_Regions.shp -progress\n",
    "* ogr2ogr -f GeoJSON Cohort2006TEARegionPoints.json Data/FinalShapefiles/ESC_Points.shp -progress\n",
    "* tippecanoe --output=Cohort2006TEARegionData.mbtiles Cohort2006TEARegionPoints.json Cohort2006TEARegionPolys.json -r1 --drop-fraction-as-needed  --simplification=9 --maximum-zoom=15 --minimum-zoom=3 --exclude=OBJECTID_1 --detect-shared-borders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
